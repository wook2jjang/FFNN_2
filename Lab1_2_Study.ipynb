{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5453e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a187f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h1, dim_h2, dim_out):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        # 이번엔 모델에서 softmax를 빼고, 뒤의 loss에서 cross_entropy loss를 활용해봅시다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859454b",
   "metadata": {},
   "source": [
    "train() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb465c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbde005",
   "metadata": {},
   "source": [
    "evaluate() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba5acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f95e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e756d2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wook\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173a13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0ec2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent 방식\n",
    "model_sgd = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model_sgd = model_sgd.to(device)\n",
    "\n",
    "# adagrad 방식\n",
    "model_ada = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model_ada = model_ada.to(device)\n",
    "\n",
    "# RMSProp 방식\n",
    "model_rms = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model_rms = model_rms.to(device)\n",
    "\n",
    "# adam 방식\n",
    "model_adam = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model_adam = model_adam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8d76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd69dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252e36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd2cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa194ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sgd = optim.SGD(model_sgd.parameters(), lr = LR)\n",
    "opt_ada = optim.Adagrad(model_ada.parameters(), lr = LR)\n",
    "opt_rms = optim.RMSprop(model_rms.parameters(), lr = LR)\n",
    "opt_adam = optim.Adam(model_adam.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bbfa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'SGD': {'model': model_sgd, 'optimizer': opt_sgd},\n",
    "    'Adagrad': {'model': model_ada, 'optimizer': opt_ada},\n",
    "    'RMSProp': {'model': model_rms, 'optimizer': opt_rms},\n",
    "    'Adam': {'model': model_adam, 'optimizer': opt_adam},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e84c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer: SGD\n",
      "Epoch: 01 | Time: 0m 5s\n",
      "\tTrain Loss: 1.497 | Test Loss: 0.609 | Test Acc: 83.730% \n",
      "Epoch: 02 | Time: 0m 9s\n",
      "\tTrain Loss: 0.477 | Test Loss: 0.374 | Test Acc: 89.240% \n",
      "Epoch: 03 | Time: 0m 13s\n",
      "\tTrain Loss: 0.366 | Test Loss: 0.332 | Test Acc: 90.620% \n",
      "Epoch: 04 | Time: 0m 17s\n",
      "\tTrain Loss: 0.324 | Test Loss: 0.296 | Test Acc: 91.660% \n",
      "Epoch: 05 | Time: 0m 20s\n",
      "\tTrain Loss: 0.297 | Test Loss: 0.273 | Test Acc: 92.490% \n",
      "Epoch: 06 | Time: 0m 24s\n",
      "\tTrain Loss: 0.276 | Test Loss: 0.261 | Test Acc: 92.610% \n",
      "Epoch: 07 | Time: 0m 28s\n",
      "\tTrain Loss: 0.260 | Test Loss: 0.244 | Test Acc: 93.010% \n",
      "Epoch: 08 | Time: 0m 32s\n",
      "\tTrain Loss: 0.243 | Test Loss: 0.230 | Test Acc: 93.390% \n",
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 0.229 | Test Loss: 0.218 | Test Acc: 93.510% \n",
      "Epoch: 10 | Time: 0m 40s\n",
      "\tTrain Loss: 0.216 | Test Loss: 0.205 | Test Acc: 93.880% \n",
      "optimizer: Adagrad\n",
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 2.250 | Test Loss: 2.206 | Test Acc: 51.900% \n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 2.174 | Test Loss: 2.137 | Test Acc: 55.770% \n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 2.111 | Test Loss: 2.075 | Test Acc: 58.520% \n",
      "Epoch: 04 | Time: 0m 16s\n",
      "\tTrain Loss: 2.052 | Test Loss: 2.016 | Test Acc: 61.550% \n",
      "Epoch: 05 | Time: 0m 20s\n",
      "\tTrain Loss: 1.996 | Test Loss: 1.960 | Test Acc: 64.130% \n",
      "Epoch: 06 | Time: 0m 24s\n",
      "\tTrain Loss: 1.942 | Test Loss: 1.906 | Test Acc: 66.300% \n",
      "Epoch: 07 | Time: 0m 28s\n",
      "\tTrain Loss: 1.890 | Test Loss: 1.854 | Test Acc: 67.940% \n",
      "Epoch: 08 | Time: 0m 32s\n",
      "\tTrain Loss: 1.840 | Test Loss: 1.804 | Test Acc: 69.310% \n",
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 1.792 | Test Loss: 1.757 | Test Acc: 70.490% \n",
      "Epoch: 10 | Time: 0m 40s\n",
      "\tTrain Loss: 1.747 | Test Loss: 1.712 | Test Acc: 71.750% \n",
      "optimizer: RMSProp\n",
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 1.105 | Test Loss: 0.568 | Test Acc: 86.910% \n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 0.480 | Test Loss: 0.392 | Test Acc: 89.440% \n",
      "Epoch: 03 | Time: 0m 11s\n",
      "\tTrain Loss: 0.372 | Test Loss: 0.331 | Test Acc: 90.850% \n",
      "Epoch: 04 | Time: 0m 16s\n",
      "\tTrain Loss: 0.325 | Test Loss: 0.297 | Test Acc: 91.470% \n",
      "Epoch: 05 | Time: 0m 20s\n",
      "\tTrain Loss: 0.296 | Test Loss: 0.276 | Test Acc: 92.070% \n",
      "Epoch: 06 | Time: 0m 24s\n",
      "\tTrain Loss: 0.276 | Test Loss: 0.260 | Test Acc: 92.540% \n",
      "Epoch: 07 | Time: 0m 28s\n",
      "\tTrain Loss: 0.259 | Test Loss: 0.247 | Test Acc: 92.870% \n",
      "Epoch: 08 | Time: 0m 33s\n",
      "\tTrain Loss: 0.244 | Test Loss: 0.235 | Test Acc: 93.210% \n",
      "Epoch: 09 | Time: 0m 37s\n",
      "\tTrain Loss: 0.231 | Test Loss: 0.224 | Test Acc: 93.500% \n",
      "Epoch: 10 | Time: 0m 41s\n",
      "\tTrain Loss: 0.220 | Test Loss: 0.215 | Test Acc: 93.690% \n",
      "optimizer: Adam\n",
      "Epoch: 01 | Time: 0m 3s\n",
      "\tTrain Loss: 1.880 | Test Loss: 1.158 | Test Acc: 77.790% \n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 0.768 | Test Loss: 0.518 | Test Acc: 87.610% \n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 0.456 | Test Loss: 0.384 | Test Acc: 89.750% \n",
      "Epoch: 04 | Time: 0m 17s\n",
      "\tTrain Loss: 0.367 | Test Loss: 0.327 | Test Acc: 91.130% \n",
      "Epoch: 05 | Time: 0m 21s\n",
      "\tTrain Loss: 0.323 | Test Loss: 0.295 | Test Acc: 91.630% \n",
      "Epoch: 06 | Time: 0m 26s\n",
      "\tTrain Loss: 0.295 | Test Loss: 0.274 | Test Acc: 92.220% \n",
      "Epoch: 07 | Time: 0m 30s\n",
      "\tTrain Loss: 0.275 | Test Loss: 0.258 | Test Acc: 92.700% \n",
      "Epoch: 08 | Time: 0m 34s\n",
      "\tTrain Loss: 0.259 | Test Loss: 0.244 | Test Acc: 93.270% \n",
      "Epoch: 09 | Time: 0m 38s\n",
      "\tTrain Loss: 0.246 | Test Loss: 0.235 | Test Acc: 93.240% \n",
      "Epoch: 10 | Time: 0m 42s\n",
      "\tTrain Loss: 0.235 | Test Loss: 0.224 | Test Acc: 93.620% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "keys = ['SGD', 'Adagrad', 'RMSProp', 'Adam']\n",
    "result_dict = {\n",
    "    'SGD': {'trn_loss': [], 'val_loss': [], 'val_acc': []},\n",
    "    'Adagrad': {'trn_loss': [], 'val_loss': [], 'val_acc': []},\n",
    "    'RMSProp': {'trn_loss': [], 'val_loss': [], 'val_acc': []},\n",
    "    'Adam': {'trn_loss': [], 'val_loss': [], 'val_acc': []},\n",
    "}\n",
    "\n",
    "for key in keys:\n",
    "    print(f'optimizer: {key}')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        trn_loss = train(model=param_dict[key]['model'], \n",
    "                         data_loader=trn_loader, \n",
    "                         optimizer=param_dict[key]['optimizer'], \n",
    "                         criterion=loss_func,\n",
    "                         device=device)\n",
    "\n",
    "        val_loss, accuracy = evaluate(model=param_dict[key]['model'], \n",
    "                                      data_loader=tst_loader, \n",
    "                                      optimizer=param_dict[key]['optimizer'], \n",
    "                                      criterion=loss_func,\n",
    "                                      device=device)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    #     if val_loss < best_val_loss:\n",
    "    #         best_val_loss = val_loss\n",
    "    #         torch.save(model.state_dict(), f'{save_dir}/my_model.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')\n",
    "\n",
    "        result_dict[key]['trn_loss'] = trn_loss\n",
    "        result_dict[key]['val_loss'] = val_loss\n",
    "        result_dict[key]['val_acc'] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab0ed6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd33328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2729e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = nn.Linear(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bfcf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4645,  0.1620,  0.3790],\n",
       "         [-0.0798, -0.2333, -0.0215]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3341, -0.0347], requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in fc1.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3da5576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4645,  0.1620,  0.3790],\n",
       "        [-0.0798, -0.2333, -0.0215]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c55987d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3341, -0.0347], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b23a5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 분포에서 random하게 샘플링해서 초기화\n",
    "nn.init.normal_(fc1.weight, mean=0.0, std=1.0)\n",
    "\n",
    "# 특정 값으로 초기화\n",
    "nn.init.zeros_(fc1.bias)\n",
    "nn.init.constant_(fc1.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def7fe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3941, -0.6126, -1.4673],\n",
       "        [ 1.0696, -0.2772, -0.0807]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f08b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3721d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tensor = torch.tensor([[1.,2.,3.],[4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2dc0882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight.data = tmp_tensor\n",
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b56379d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0227,  0.3041, -0.3069],\n",
       "        [ 0.8272,  0.5798, -0.5675]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4061105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9145,  0.2248,  0.1328],\n",
       "        [-1.2407, -1.6195, -0.0074]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.kaiming_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b9f03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 그 즉시 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8787e56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MyNet(\n",
       "   (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
       "   (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
       " ),\n",
       " Linear(in_features=784, out_features=50, bias=True),\n",
       " Linear(in_features=50, out_features=100, bias=True),\n",
       " Linear(in_features=100, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model의 module들은 뭐가 있는지 봅시다.\n",
    "model = MyNet()\n",
    "[x for x in model.modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e80f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b0ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0c6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fac33e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wook\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67df7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c16aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d905a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd56570",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd19420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1374109",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10e7eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9183ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 1.434 | Test Loss: 0.702 | Test Acc: 83.020% \n",
      "Epoch: 02 | Time: 0m 3s\n",
      "\tTrain Loss: 0.523 | Test Loss: 0.391 | Test Acc: 89.990% \n",
      "Epoch: 03 | Time: 0m 3s\n",
      "\tTrain Loss: 0.359 | Test Loss: 0.312 | Test Acc: 91.560% \n",
      "Epoch: 04 | Time: 0m 3s\n",
      "\tTrain Loss: 0.302 | Test Loss: 0.274 | Test Acc: 92.320% \n",
      "Epoch: 05 | Time: 0m 3s\n",
      "\tTrain Loss: 0.270 | Test Loss: 0.251 | Test Acc: 92.880% \n",
      "Epoch: 06 | Time: 0m 3s\n",
      "\tTrain Loss: 0.246 | Test Loss: 0.230 | Test Acc: 93.510% \n",
      "Epoch: 07 | Time: 0m 3s\n",
      "\tTrain Loss: 0.229 | Test Loss: 0.216 | Test Acc: 93.820% \n",
      "Epoch: 08 | Time: 0m 3s\n",
      "\tTrain Loss: 0.214 | Test Loss: 0.206 | Test Acc: 94.060% \n",
      "Epoch: 09 | Time: 0m 3s\n",
      "\tTrain Loss: 0.201 | Test Loss: 0.196 | Test Acc: 94.250% \n",
      "Epoch: 10 | Time: 0m 3s\n",
      "\tTrain Loss: 0.191 | Test Loss: 0.187 | Test Acc: 94.400% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model2.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948626e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e54ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885eeb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188ebebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd04178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet_BN(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_h1) # input -> hidden1로 가는 과정에서 필요한 batchnorm layer\n",
    "        self.bn2 = nn.BatchNorm1d(dim_h2) # hidden1 -> hidden2로 가는 과정에서 필요한 batchnorm layer\n",
    "#         self.bn3 = nn.BatchNorm1d(10) # output layer에서는 batchnorm이 통상적으로 잘 사용되지 않는 것 같습니다.\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = self.bn1(h1) # batchnorm은 affine연산(matrix multiplication)이후 사용.(activation전에!)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = self.bn2(h2) # batchnorm은 affine연산(matrix multiplication)이후 사용.(activation전에!)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfac97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로! BN이 있는 모델을 학습할 때, 반드시 필요함\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206d2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로! BN이 있는 모델을 학습할 때, 반드시 필요함\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f33d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0fe363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wook\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5d5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d6e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batchnorm\n",
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)\n",
    "\n",
    "# with batchnorm\n",
    "model_bn = MyNet_BN(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model_bn = model_bn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d1bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c5a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb03a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351acdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d72a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)\n",
    "my_opt_bn = optim.Adam(model_bn.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b655303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 1.511 | Test Loss: 0.750 | Test Acc: 83.210% \n",
      "Epoch: 02 | Time: 0m 4s\n",
      "\tTrain Loss: 0.550 | Test Loss: 0.401 | Test Acc: 89.950% \n",
      "Epoch: 03 | Time: 0m 4s\n",
      "\tTrain Loss: 0.364 | Test Loss: 0.307 | Test Acc: 91.860% \n",
      "Epoch: 04 | Time: 0m 3s\n",
      "\tTrain Loss: 0.297 | Test Loss: 0.265 | Test Acc: 92.690% \n",
      "Epoch: 05 | Time: 0m 4s\n",
      "\tTrain Loss: 0.260 | Test Loss: 0.238 | Test Acc: 93.120% \n",
      "Epoch: 06 | Time: 0m 3s\n",
      "\tTrain Loss: 0.235 | Test Loss: 0.218 | Test Acc: 93.700% \n",
      "Epoch: 07 | Time: 0m 4s\n",
      "\tTrain Loss: 0.215 | Test Loss: 0.204 | Test Acc: 93.990% \n",
      "Epoch: 08 | Time: 0m 3s\n",
      "\tTrain Loss: 0.200 | Test Loss: 0.190 | Test Acc: 94.450% \n",
      "Epoch: 09 | Time: 0m 3s\n",
      "\tTrain Loss: 0.187 | Test Loss: 0.182 | Test Acc: 94.660% \n",
      "Epoch: 10 | Time: 0m 4s\n",
      "\tTrain Loss: 0.176 | Test Loss: 0.172 | Test Acc: 94.850% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model3_1.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f589911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 3s\n",
      "\tTrain Loss: 1.369 | Test Loss: 0.725 | Test Acc: 84.160% \n",
      "Epoch: 02 | Time: 0m 3s\n",
      "\tTrain Loss: 0.572 | Test Loss: 0.428 | Test Acc: 90.120% \n",
      "Epoch: 03 | Time: 0m 4s\n",
      "\tTrain Loss: 0.386 | Test Loss: 0.319 | Test Acc: 92.200% \n",
      "Epoch: 04 | Time: 0m 4s\n",
      "\tTrain Loss: 0.305 | Test Loss: 0.265 | Test Acc: 93.180% \n",
      "Epoch: 05 | Time: 0m 4s\n",
      "\tTrain Loss: 0.257 | Test Loss: 0.233 | Test Acc: 93.740% \n",
      "Epoch: 06 | Time: 0m 4s\n",
      "\tTrain Loss: 0.226 | Test Loss: 0.208 | Test Acc: 94.320% \n",
      "Epoch: 07 | Time: 0m 4s\n",
      "\tTrain Loss: 0.202 | Test Loss: 0.190 | Test Acc: 94.690% \n",
      "Epoch: 08 | Time: 0m 4s\n",
      "\tTrain Loss: 0.183 | Test Loss: 0.175 | Test Acc: 95.090% \n",
      "Epoch: 09 | Time: 0m 4s\n",
      "\tTrain Loss: 0.168 | Test Loss: 0.163 | Test Acc: 95.320% \n",
      "Epoch: 10 | Time: 0m 4s\n",
      "\tTrain Loss: 0.154 | Test Loss: 0.155 | Test Acc: 95.510% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model_bn, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt_bn, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model_bn, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt_bn, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model_bn.state_dict(), f'{save_dir}/my_model3_2.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291b1a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcf45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9b1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h1, dim_h2, dim_out):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        # 이번엔 모델에서 softmax를 빼고, 뒤의 loss에서 cross_entropy loss를 활용해봅시다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908cd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa86204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64baaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f46f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wook\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c0ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441e0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f824fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**9\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5572932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(my_opt, step_size=10, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1349fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 5 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 6 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 7 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 8 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 9 [50688/60000 (84%)]\tLoss: nan\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: nan\n",
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = y_batch_prob.argmax(dim=1)  # 수정된 부분\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.cpu().numpy())  # 수정된 부분\n",
    "            y_real_list.append(y_batch.cpu().numpy())  # 수정된 부분\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f21783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
