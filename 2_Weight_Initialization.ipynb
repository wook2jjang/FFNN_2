{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2a6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95f803",
   "metadata": {},
   "source": [
    "## Initialization을 사용하는 방법: torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = nn.Linear(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a253ee8",
   "metadata": {},
   "source": [
    "### 각 모듈별로 weight나 bias객체 속성에 실제값이 들어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c802a2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2960, -0.4949, -0.4719],\n",
       "         [ 0.4876, -0.0733,  0.0737]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4311, 0.5641], requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in fc1.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc73e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2960, -0.4949, -0.4719],\n",
       "        [ 0.4876, -0.0733,  0.0737]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112f177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4311, 0.5641], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648cfa3",
   "metadata": {},
   "source": [
    "### nn.init에는 여러가지 종류가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761d0dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 분포에서 random하게 샘플링해서 초기화\n",
    "nn.init.normal_(fc1.weight, mean=0.0, std=1.0)\n",
    "\n",
    "# 특정 값으로 초기화\n",
    "nn.init.zeros_(fc1.bias)\n",
    "nn.init.constant_(fc1.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3831cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5822,  1.8538, -2.3147],\n",
       "        [ 0.7066, -0.1409,  0.6233]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0db2567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a5cce",
   "metadata": {},
   "source": [
    "### 직접 값을 지정하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff7fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tensor = torch.tensor([[1.,2.,3.],[4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4379e7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight.data = tmp_tensor\n",
    "fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e1a14",
   "metadata": {},
   "source": [
    "## Xavier Initialization & He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238811bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2401, -0.5857, -0.4451],\n",
       "        [-0.0358, -1.4800, -0.8516]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8dce0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2901, -0.5437,  0.8363],\n",
       "        [-1.3419,  0.1662,  0.1981]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.kaiming_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360c6ae",
   "metadata": {},
   "source": [
    "# 실제 모델에 적용해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafa95a",
   "metadata": {},
   "source": [
    "## 1. Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec94b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 그 즉시 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        out = self.fc3(h2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7dfc9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MyNet(\n",
       "   (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
       "   (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
       "   (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
       " ),\n",
       " Linear(in_features=784, out_features=50, bias=True),\n",
       " Linear(in_features=50, out_features=100, bias=True),\n",
       " Linear(in_features=100, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model의 module들은 뭐가 있는지 봅시다.\n",
    "model = MyNet()\n",
    "[x for x in model.modules()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7d6c6",
   "metadata": {},
   "source": [
    "## 2. train() 함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`train()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 모델에 입력하기 적합하도록 x의 형태를 변경하고 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 지난 batch로부터 계산했던 gradient를 초기화(`zero_grad()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Backpropagation으로 각 parameter의 gradient를 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> Gradient Descent로 parameter값 update</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> `trn_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 8.</b> 데이터 한 개당 평균 train loss 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8973516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7623bb8",
   "metadata": {},
   "source": [
    "## 3. evaluate()함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`evaluate()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 모델에 입력하기 적합하도록 x의 형태를 변경하고 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> 모델이 예측하는 레이블을 산출 (with `torch.argmax()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Minibatch의 실제 레이블(`y`)과 예측 레이블(`y_pred_label`)을 누적하여 저장</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> `eval_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> 데이터 한 개당 평균 evaluation loss와 accuracy 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e2467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a814fa",
   "metadata": {},
   "source": [
    "## 4. 매 Epoch에 드는 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46bf149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bfc79",
   "metadata": {},
   "source": [
    "## 5. 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49af93",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Dataset과 Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec712f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8d919",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>연산을 수행할 device를 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a53b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347714d",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>모델에 대한 객체 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777afa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a273dbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>학습한 모델을 저장할 directory 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aac3ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2488cb",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요한 hyperparameter값 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac13e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685a3ce",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>loss function 정의하기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>예전 실습 파일에는 손실 함수인 F.nll_loss()가 train(), evaluate() 함수 안에서 바로 사용되었음</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>이번 손실 함수는 train(), evaluate() 함수 밖에서 정의하고 안에서는 정의한 함수를 사용하는 방식으로</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>어떻게 사용해도 상관없으니, 편한대로 사용하세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4383fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b391e05",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e45c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d6347",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>optimizer 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5067d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaecbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "533740db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 1.412 | Test Loss: 0.711 | Test Acc: 82.760% \n",
      "Epoch: 02 | Time: 0m 12s\n",
      "\tTrain Loss: 0.540 | Test Loss: 0.406 | Test Acc: 89.480% \n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 0.371 | Test Loss: 0.324 | Test Acc: 91.240% \n",
      "Epoch: 04 | Time: 0m 12s\n",
      "\tTrain Loss: 0.310 | Test Loss: 0.282 | Test Acc: 92.180% \n",
      "Epoch: 05 | Time: 0m 12s\n",
      "\tTrain Loss: 0.275 | Test Loss: 0.256 | Test Acc: 93.020% \n",
      "Epoch: 06 | Time: 0m 12s\n",
      "\tTrain Loss: 0.251 | Test Loss: 0.237 | Test Acc: 93.290% \n",
      "Epoch: 07 | Time: 0m 12s\n",
      "\tTrain Loss: 0.232 | Test Loss: 0.224 | Test Acc: 93.610% \n",
      "Epoch: 08 | Time: 0m 12s\n",
      "\tTrain Loss: 0.216 | Test Loss: 0.209 | Test Acc: 93.950% \n",
      "Epoch: 09 | Time: 0m 12s\n",
      "\tTrain Loss: 0.203 | Test Loss: 0.199 | Test Acc: 94.260% \n",
      "Epoch: 10 | Time: 0m 12s\n",
      "\tTrain Loss: 0.192 | Test Loss: 0.189 | Test Acc: 94.620% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model2.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a148b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863733e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02920ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
