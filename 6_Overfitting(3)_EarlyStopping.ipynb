{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a24bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48599d",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "\n",
    "- <span style = 'font-size:1.3em;line-height:1.5em'>Early Stopping은 다음과 같은 방식으로 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>(1) 매 epoch마다 train을 진행합니다.</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>(2) 한 epoch에 대해 train이 끝나면 validation set에서 현재까지 학습된 모델로 loss를 계산합니다.</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>val_loss = loss_func(y_val, y_val_est)</span>\n",
    "\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>(3) 현재의 validation loss가 이제까지의 validation loss의 최소값보다 연속으로 n번 크게 되면 학습을 멈춘다</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>val_loss > best_val_loss (n consecutive times) --> stop training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0f417",
   "metadata": {},
   "source": [
    "## 실험을 CPU에서? GPU에서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4001f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27537286",
   "metadata": {},
   "source": [
    "## 1. 모델을 클래스 형식으로 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efb24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in,dim_h1)\n",
    "        self.fc2 = nn.Linear(dim_h1,dim_h2)\n",
    "        self.fc3 = nn.Linear(dim_h2,dim_out)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = F.relu(h1)\n",
    "        \n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "        \n",
    "        out = self.fc3(h2)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5302cf",
   "metadata": {},
   "source": [
    "## 2. train() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5633f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로! Dropout이 있는 모델을 학습할 때, 반드시 필요함\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc44f1",
   "metadata": {},
   "source": [
    "## 3. evaluate() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12144e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로! Dropout이 있는 모델을 학습할 때, 반드시 필요함\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a814fa",
   "metadata": {},
   "source": [
    "## 4. 매 Epoch에 드는 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46bf149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bfc79",
   "metadata": {},
   "source": [
    "## 5. 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49af93",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Dataset과 Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec712f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8d919",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>연산을 수행할 device를 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a53b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347714d",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>모델에 대한 객체 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777afa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batchnorm\n",
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701a291",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>loss function 정의하기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>예전 실습 파일에는 손실 함수인 F.nll_loss()가 train(), evaluate() 함수 안에서 바로 사용되었음</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>이번 손실 함수는 train(), evaluate() 함수 밖에서 정의하고 안에서는 정의한 함수를 사용하는 방식으로</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>어떻게 사용해도 상관없으니, 편한대로 사용하세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdfbdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a273dbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>학습한 모델을 저장할 directory 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac3ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2488cb",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요한 hyperparameter값 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac13e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b391e05",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e45c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d6347",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>optimizer 생성하기 <b>(weight decay부분이 여기 들어갑니다.)</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5067d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaecbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2bb06eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 3s\n",
      "\tTrn Loss: 0.299 | Val Loss: 0.159 | Val Acc: 95.060% \n",
      "\tval_loss(0.159) < best_val_loss(inf)\n",
      "\tkeep training, best_val_loss is replaced to 0.159\n",
      "Epoch: 02 | Time: 0m 7s\n",
      "\tTrn Loss: 0.131 | Val Loss: 0.137 | Val Acc: 95.980% \n",
      "\tval_loss(0.137) < best_val_loss(0.159)\n",
      "\tkeep training, best_val_loss is replaced to 0.137\n",
      "Epoch: 03 | Time: 0m 16s\n",
      "\tTrn Loss: 0.099 | Val Loss: 0.128 | Val Acc: 96.090% \n",
      "\tval_loss(0.128) < best_val_loss(0.137)\n",
      "\tkeep training, best_val_loss is replaced to 0.128\n",
      "Epoch: 04 | Time: 0m 22s\n",
      "\tTrn Loss: 0.081 | Val Loss: 0.102 | Val Acc: 96.910% \n",
      "\tval_loss(0.102) < best_val_loss(0.128)\n",
      "\tkeep training, best_val_loss is replaced to 0.102\n",
      "Epoch: 05 | Time: 0m 22s\n",
      "\tTrn Loss: 0.069 | Val Loss: 0.105 | Val Acc: 96.940% \n",
      "\tval_loss(0.105) >= best_val_loss(0.102)\n",
      "\tModel is not updated and n_violence is increased. n_violence=1\n",
      "Epoch: 06 | Time: 0m 21s\n",
      "\tTrn Loss: 0.060 | Val Loss: 0.134 | Val Acc: 96.330% \n",
      "\tval_loss(0.134) >= best_val_loss(0.102)\n",
      "\tModel is not updated and n_violence is increased. n_violence=2\n",
      "Epoch: 07 | Time: 0m 22s\n",
      "\tTrn Loss: 0.053 | Val Loss: 0.111 | Val Acc: 96.730% \n",
      "\tval_loss(0.111) >= best_val_loss(0.102)\n",
      "\tModel is not updated and n_violence is increased. n_violence=3\n",
      ">> n_violence=3. Stop training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "n_patience = 3\n",
    "n_violence = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrn Loss: {trn_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {100*accuracy:.3f}% ')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'\\tval_loss({val_loss:.3f}) < best_val_loss({best_val_loss:.3f})')\n",
    "        print(f'\\tkeep training, best_val_loss is replaced to {val_loss:.3f}')\n",
    "        best_val_loss = val_loss\n",
    "        n_violence = 0\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model6.pt')\n",
    "    \n",
    "    else:\n",
    "        n_violence+=1\n",
    "        print(f'\\tval_loss({val_loss:.3f}) >= best_val_loss({best_val_loss:.3f})')\n",
    "        print(f'\\tModel is not updated and n_violence is increased. n_violence={n_violence}')\n",
    "        \n",
    "        \n",
    "    if n_violence >= n_patience:\n",
    "        print(f'>> n_violence={n_patience}. Stop training!\\n')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b93a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
